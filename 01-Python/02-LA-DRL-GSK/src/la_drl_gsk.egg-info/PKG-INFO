Metadata-Version: 2.1
Name: la-drl-gsk
Version: 2.0.0
Summary: Landscape-Aware Deep Reinforcement Learning GSK Algorithm
Home-page: https://github.com/your-repo/LA-DRL-GSK
Author: LA-DRL-GSK Research Team
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Provides-Extra: all
Provides-Extra: dev
Provides-Extra: rl
Provides-Extra: torch
Provides-Extra: viz

# LA-DRL-GSK: Landscape-Aware Deep Reinforcement Learning for GSK

Q1 Implementation of Landscape-Aware Deep Reinforcement Learning controller for the Gaining-Sharing Knowledge (GSK) optimization algorithm.

## Features

- **Baseline GSK**: Pure NumPy implementation of original GSK algorithm
- **LA-DRL-GSK Q1**: RL-controlled GSK with windowed parameter adaptation
- **Zero-Cost FLA**: 25 landscape features computed from population only (no extra evaluations)
- **Controller Backends**: Fixed, Heuristic (NumPy-only), SB3 PPO (optional)
- **CEC2017 Benchmarks**: Full support with external or bundled implementation

## Installation

### Minimal (NumPy-only)

```bash
pip install -e .
# or
pip install -r requirements-minimal.txt
```

### With RL Support (gymnasium + stable-baselines3)

```bash
pip install -e ".[rl]"
# or
pip install -r requirements-rl.txt
```

## Quick Start

### Demo

```bash
python run.py demo
```

### Baseline GSK (no RL)

```python
from la_drl_gsk import LADRLGSK, LADRLGSKConfig

def sphere(x):
    import numpy as np
    return np.sum(x ** 2, axis=1)

config = LADRLGSKConfig(dim=30, use_rl=False)
optimizer = LADRLGSK(config)
result = optimizer.optimize(sphere)
print(f"Best fitness: {result.best_f:.6e}")
```

### LA-DRL-GSK with Heuristic Controller (NumPy-only)

```python
config = LADRLGSKConfig(
    dim=30,
    use_rl=True,
    controller_backend="heuristic",
    control_window=5,
)
optimizer = LADRLGSK(config)
result = optimizer.optimize(sphere)
```

### LA-DRL-GSK with Trained SB3 Policy

```python
config = LADRLGSKConfig(
    dim=30,
    use_rl=True,
    controller_backend="sb3",
    policy_path="models/ppo_gsk_D30.zip",
)
optimizer = LADRLGSK(config)
result = optimizer.optimize(sphere)
```

## Training RL Controller

### Using run.py

```bash
python run.py train --dims 30 --timesteps 200000 --n-envs 8
```

### Using train_sb3.py directly

```bash
python -m la_drl_gsk.train_sb3 --dim 30 --timesteps 200000 --save-dir models
```

## Evaluation

```bash
python run.py evaluate --dims 10 30 --runs 51 --model models/ppo_gsk_D30.zip
```

## Q1 Design

### Action Space

The RL controller outputs 4 absolute GSK parameters:

| Parameter | Range | Description |
|-----------|-------|-------------|
| K | [1, 20] | Knowledge rate exponent (log-scaled) |
| kf | [0.05, 1.0] | Knowledge factor |
| kr | [0.05, 0.99] | Knowledge ratio |
| p | [0.05, 0.20] | Senior stratification fraction |

### Observation Space

25 zero-cost FLA features computed from population and fitness:

- **Population (5)**: diversity, spread, centroid_offset, variance, elite_clustering
- **Fitness (5)**: range, cv, skewness, kurtosis, elite_gap
- **Correlation (5)**: fdc, fdc_centroid, tau, separability, neighbor_correlation
- **Temporal (5)**: improvement_rate, stagnation, div_trend, mean_trend, consistency
- **Progress (5)**: progress, convergence_ratio, exploitation_mode, improvement_potential, quality

### Windowed Control

- RL decision every W generations (default W=5)
- Parameters held constant within window
- Reward: relative best improvement over window

## Project Structure

```
02-LA-DRL-GSK/
├── src/
│   ├── la_drl_gsk/
│   │   ├── la_drl_gsk.py       # Main optimizer
│   │   ├── controllers.py       # Fixed, Heuristic, SB3 controllers
│   │   ├── landscape_analyzer.py # 25 FLA features
│   │   ├── gsk_env.py          # Gymnasium environment
│   │   ├── train_sb3.py        # SB3 PPO training
│   │   ├── experiments.py      # Evaluation utilities
│   │   └── cec2017_benchmark.py # CEC2017 interface
│   ├── gsk/                    # Baseline GSK implementation
│   └── cec2017/               # Bundled CEC2017 (simplified)
├── tests/
├── models/                    # Trained models
├── results/                   # Evaluation results
├── run.py                     # CLI entry point
├── requirements-minimal.txt   # NumPy-only deps
├── requirements-rl.txt        # RL deps (gymnasium, SB3)
└── setup.py
```

## Using External CEC2017

For official CEC2017 benchmarks, place the 00-CEC-Root directory as a sibling:

```
parent_dir/
├── 00-CEC-Root/
│   └── cec2017/
│       └── functions.py
└── 02-LA-DRL-GSK/
```

Or specify the path:

```bash
python run.py demo --cec-path /path/to/00-CEC-Root
```

## Testing

```bash
# Minimal tests (no RL deps)
python -m pytest tests/ -v

# All tests (requires RL deps)
pip install -e ".[rl]"
python -m pytest tests/ -v
```

## Citation

If you use this code, please cite:

```bibtex
@article{ladrlgsk2025,
  title={LA-DRL-GSK: Landscape-Aware Deep Reinforcement Learning for GSK},
  author={LA-DRL-GSK Research Team},
  year={2025}
}
```

## License

MIT License
